---
title: 'Large Language Models Concepts'
date: 2024-01-18
permalink: /posts/2024/01/blog-post-2/
tags:
  - LLM
---
LLM courses' notes




Chapter 1 : Introduction to Large Language Models (LLM)
======

1 The rise of LLMs in AI landscape
======
AI has performed well in data-driven tasks such as sentiment analysis, fraud detection and more. However, it traditionally lacked ability to understand context, respond to open-ended questions, and generate human-like responses in conversation. Recent developments in language-based AI models have led to a disruptive breakthrough. These models, called Large Language Models, can process language as humans do. 

LLMs use deep learning techniques to perform a variety of NLP (Natural Language Processing (NLP) is the technology that allows computers to understand, interpret, and generate human language in a meaningful way. This technology is often employed in applications such as translation services and chatbots.) tasks such as text classification, summarization, generation and more.  

Definitions of LLMs : 
------
Large : training data and resources 
Language : they are powerful in processing and analyze human language data
Models : Learn complex patterns using text data 

The defining moment : 
------
The LLM is considered the defining moment. 
GPT series is one of the most popular language models among the family of LLMs, primarily because of its ability to engage in rich human interactions. 

Applications :
------
Sentiment analysis, Identifying themes, Translating speech or text, Generating codes, Next-word prediction. 

Objectives of this course
------
- Conceptual understanding of LLMs
- Training data considerations
- Ethical, privacy and environmental concerns
- wrap up this course by covering the enhancements and future work of LLMs 


2 Real-world applications :
======
This section aims to understand business opportunities and benefits of leveraging LLMs. 

Business opportunies
------
Benefits : Automate tasks, Improve efficiency, create revenue streams, enable new capacities. (自动化任务、提高效率、创造收入流、启用新能力。)

Transforming finance industry
------
Financial analysis of a company can be complex and may include processing unstructured text such as investement outlooks, annual reports, new articles, and socia media posts. 

**Unstructured data or text** : data that lacks definition and is presented free-form. 
LLM can analyze such data to generate valuable insights into market trends, manage investments, and identify new investment opportunities. 

Challenges in healthcare 
------
Analyzing health records is important for giving personalized recommendations to provide quality healthcare. But, much of thees information is in doctors' notes, which can be hard to understand because they use **jargon and abbrevations**. Further more, **domain specific-knowledge and varying writing styles** among practitioners add to the the challenges of interpreting this critical information effectively. Processing such varied data and understanding complex **acronyms** makes it difficult to have a general system to describe any patient files. 

Revolutionizing healthcare sector 
------
- LLMs can analyze patient data to offer personalized recommendations : medical records, health check-out results, imaging reports, to provide personalized treatment recommendations. 
- Patient data is private and personnal, so anyone use LLM must adhere to privacy laws.

Education 
------
- Personnalized coaching and feedback 
- Interactive learning experience
- AI-powered tutor can ask questions, recevie guidance, discuss ideas
- Moreover, the tutor can adapt its teaching style based on the learner's conceptual understanding.

Multimodol applications of LLM 
------
Visual question answering : process both image and text data to generate responses. 

3 Challenges of language modeling 
======

Challenges of language modeling
------
1. **Sequence matters** : Modeling a language requires understanding the sequential nature of text, because placing even one word differently can change the meaning of the sentence completely.
2. **Context Modeling** : language is highly contextual, meaning the same word can have different meanings depending on the context in which it is used. To accurately model language, language models must analyze and interpret the surrounding words, phrases, and sentences to identify the most likely meaning of a given word.
3. **Long-range dependency**

Single-task learning
------
For example : 

Task 1 model --> Question Answering 

Task 2 model --> Text Summarization 

Tasl 3 model --> Translation 

**Issues** : Time and resource expensive, less flexible compared to modern LLMs. 

Multi-task Learning 
------
With the development of LLMs, multi-task learning has become possible. This involves training a model to perform multiple related tasks simultaneously of traing separate models for each task. 
<br/><img src='/images/multitask_datacamp.png'>

Advantages : 
1. (versatile) improved performance on each individual task (using new and unseen data, but may come at the expense of accuracy and efficiency 以牺牲准确性和效率为代价) 
2. Might impact accuray and efficiency
3. it can **decrease the training data** needed for each individual task by allowing the model to learn from shared data across the tasks. 它可以通过允许模型从跨任务的共享数据中学习来减少每个单独任务所需的训练数据。

Comparison between single task and multitask learning : 
<br/><img src='/images/comparaison_single_multi.png'>


Chapter 2 : Building Blocks of LLMs 
======

1 Novelty of LLMs 
======
- LLMs Use text data : unstrutured data (messy and inconsistent) 
- Issue : Machines do not understand languages !
- Need for NLP : Text data --> NLP --> Numerical form --> Enable computers to identify the patterns and features of text data
- Unique capabilities of LLMs : abilities to detect linguisitic subtleties like irony, humor, intonation and intents !

How do LLMs understand ? 
------
1. Trained on vast amounts of data
2. Largeness of LLMs : parameters
3. Parameters represent the patterns and rules learnf from training data
4. More parameters allow for capturing more complex patterns, resulting in sophisticated and accurate responses

Emergence of new capacities 
------
These massive parameters also give rise to LLMs' emergent capabilities
1. Emergent abilities : unique to large-scale models like LLMs and not found in smaller ones. 
2. Scale : it is defined by the volume of training data and the number of model parameters (As the scale increases, the model's performance can dramatically improve beyond a certain threshold, resulting in a phase transition and a sudden emergence of new capabilities 
<br/><img src='/images/threshold_llm.png'>

3. Applications :
Music, Poetry, Code generation, Medical diagnosis and treatment plans

Buidling blocks of LLMs 
------
To reach this threshold, LLM and their parameters undergo a training process 
(**Text pre-processing, text representation, pre-training, fine-tuning, advanced fine-tuning.**)

To recap
------
- We learned how LLMs overcome data's unstructured nature
- it outperform traditional models by comprehending complex linguistic subtleties and generating detailed responses

**HOW?** : 
- this performance boost is a result of the "largeness" of LLMs, which is due to extensive training data and many parameters.
- These factors also enable "emergent abilities", which unlock advanced capabilitie and expand LLMs' use cases, making them a potent tool in Natural Language Processing. 

2 Generalized overview of NLP
======
Key NLP techniques used to prepare text data into machine-readable form for use in LLMs 

Text pre-processing 
------
It transforms raw text data into a standardized format. 

**Steps** :  Tokenization, stop word removal and lemmatication 

1. Tokenization : splits text into individual words, or tokens. After tokenization, we obtain a **list**.
2. Stop word removal : stop words do not add meaning (like "as", "with" ,"is", etc)
3. Lemmatization : group slightly different words with similar meaning. It reduces words to their base form (talking--> talk, talked --> talk)

Text representation
------
It help convert preprocessed text into a numerical form (bag-of-words, word embeddings) 

- bag-of-words : it involves converting the text into a matrix of word counts (limitations : it fails to capture the order or context, can lead to incorrect interpretations; it does not capture the semantics between the words, beacuse it treats related words as independent) 

- Word embeddings : capture the semantic meanings as numbers, allowing for similar words to have similar representations.

for example : features are "plant, furry, carnivore" 
- cat : [-0.9, 0.9, 0.9]
- mouse : [-0.8, 0.7, -0.8]
  
so cat is furry and carnivore; mouse is furry.

Machine-readable form
------
Convert pre-processed text to numerical format : text data --> tokenization stop-word removal lemmatization --> bag of words / word embedding --> NUmerical data
































